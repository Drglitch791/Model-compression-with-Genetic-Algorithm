{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# from cifar10tools import test, config_model_proper, VGG, make_layers, make_model, calculate_params, get_model_filters_config\n",
    "\n",
    "from svhn_tools import test, config_model_proper, VGG, make_layers, make_model, calculate_params, get_model_filters_config\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "ACC_THRESH = 2\n",
    "\n",
    "class Individual():\n",
    "    def __init__(self, bitstring) -> None:\n",
    "        self.bitstring = bitstring\n",
    "        self.metrics = None\n",
    "    \n",
    "    def mutate(self):\n",
    "        mp_list = [0.0002]\n",
    "        mutation_prob = mp_list[np.random.randint(len(mp_list))]\n",
    "        self.metrics = None\n",
    "        self.bitstring ^= np.random.binomial(1, mutation_prob, len(self.bitstring))\n",
    "    \n",
    "class GeneticAlgo():\n",
    "    def __init__(self,\n",
    "                 num_genes,\n",
    "                 num_parents,\n",
    "                 base_model,\n",
    "                 ) -> None:\n",
    "        self.num_genes = num_genes\n",
    "        self.num_parents = num_parents\n",
    "        self.population = [Individual(np.random.binomial(1, 0.99, num_genes)) for _ in range(50)]\n",
    "        self.base_model = base_model\n",
    "        # self.dataset_cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        # ]))\n",
    "        # self.dataset_cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        # ]))\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.dataset_train = datasets.SVHN(root='./data', split='train', transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True)\n",
    "\n",
    "        self.dataset_test = datasets.SVHN(root='./data', split='test', transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True)\n",
    "    \n",
    "    def assign_score(self, indiv, accuracy):\n",
    "        metrics = {}\n",
    "        accuracy = accuracy\n",
    "        accuracy_drop = self.base_accuracy - accuracy\n",
    "        num_filters_dropped = np.count_nonzero(indiv.bitstring == 0)\n",
    "        per_filters_dropped = num_filters_dropped * 100 / self.num_genes\n",
    "\n",
    "        if accuracy_drop < ACC_THRESH:\n",
    "            score = per_filters_dropped + 1000\n",
    "        else:\n",
    "            score = per_filters_dropped/(accuracy_drop+10) - (accuracy_drop)*100\n",
    "\n",
    "        metrics['score'] = score\n",
    "        metrics['accuracy_drop'] = accuracy_drop\n",
    "        metrics['num_filters_dropped'] = num_filters_dropped\n",
    "        metrics['per_filters_dropped'] = per_filters_dropped\n",
    "        metrics['accuracy'] = accuracy\n",
    "        indiv.metrics = metrics\n",
    "        print(metrics)\n",
    "    \n",
    "    def get_accuracy(self, indiv_batch):\n",
    "        indiv_models = [config_model_proper(indiv.bitstring, self.base_model) for indiv in indiv_batch]\n",
    "        return test(self.dataloader, indiv_models)\n",
    "    \n",
    "    def evaluate_batches(self, batches):\n",
    "        for batch in batches:\n",
    "            batch_accuracies = self.get_accuracy(batch)\n",
    "            for idx, individual in enumerate(batch):\n",
    "                accuracy = batch_accuracies[idx]\n",
    "                self.assign_score(individual, accuracy)\n",
    "    \n",
    "    def evaluate_population(self, population):\n",
    "        eval_batches = []\n",
    "        eval_batch = []\n",
    "        item_counter = 0\n",
    "        chunk_size = 100\n",
    "        for individual in population:\n",
    "            ## if the individual is not already assigned a metric\n",
    "            if not individual.metrics:\n",
    "                eval_batch.append(individual)\n",
    "                individual.metrics = {}\n",
    "                item_counter += 1\n",
    "                if item_counter % chunk_size == 0:\n",
    "                    eval_batches.append(eval_batch)\n",
    "                    eval_batch = []\n",
    "            else:\n",
    "                pass\n",
    "        if len(eval_batch) > 0 and len(eval_batch) < chunk_size:\n",
    "            eval_batches.append(eval_batch)\n",
    "        self.evaluate_batches(eval_batches)\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        '''Single Crossover'''\n",
    "        rand_point = np.random.randint(self.num_genes)\n",
    "        child1 = Individual(np.concatenate((parent1.bitstring[:rand_point], parent2.bitstring[rand_point:])))\n",
    "        child2 = Individual(np.concatenate((parent2.bitstring[:rand_point], parent1.bitstring[rand_point:])))\n",
    "        child1.mutate()\n",
    "        child2.mutate()\n",
    "        return child1, child2\n",
    "    \n",
    "    def get_next_population(self):\n",
    "        offspring_list = list()\n",
    "        sorted_score = sorted(self.population, key=lambda x: x.metrics['score'], reverse=True)[:self.num_parents]\n",
    "        for idx1 in range(len(sorted_score)-1):\n",
    "            for idx2 in range(idx1+1, len(sorted_score)):\n",
    "                offspring_list += self.crossover(sorted_score[idx1], sorted_score[idx2])\n",
    "        self.evaluate_population(offspring_list)\n",
    "        self.population += offspring_list\n",
    "\n",
    "    def run(self, save_acc_drop=[], save_fil_drop=[], save_flop_drop=[], save_params_drop=[], gen=0):\n",
    "        H, W = 86, 64\n",
    "        gen = gen\n",
    "        save_acc_drop = save_acc_drop\n",
    "        save_fil_drop = save_fil_drop\n",
    "        save_flop_drop = save_flop_drop\n",
    "        save_params_drop = save_params_drop\n",
    "\n",
    "        while True:\n",
    "            print(\"==========\")\n",
    "            print(f\"gen:{gen}, popsize:{len(self.population)}\")\n",
    "            print(\"==========\")\n",
    "            subset_indices = np.arange(len(self.dataset_train))\n",
    "            # np.random.shuffle(subset_indices)\n",
    "            ## slice subset indices to create smaller subset\n",
    "            subset = Subset(self.dataset_train, subset_indices)\n",
    "            self.dataloader = DataLoader(\n",
    "                subset,\n",
    "                batch_size=2048, shuffle=True,\n",
    "                num_workers=4, pin_memory=True)\n",
    "            self.base_accuracy = test(self.dataloader, [self.base_model])[0]\n",
    "\n",
    "            self.evaluate_population(self.population)\n",
    "            sorted_by_score = sorted(self.population, key=lambda x: x.metrics['score'], reverse=True)[:300]\n",
    "            del self.population\n",
    "            self.population = sorted_by_score\n",
    "\n",
    "            print(f\"GEN:{gen}::@@@@@@@@@@@@@@@@@@@@\")\n",
    "            print(sorted_by_score[0].metrics)\n",
    "            \n",
    "            print(sorted_by_score[1].metrics)\n",
    "            print(sorted_by_score[2].metrics)\n",
    "            print(sorted_by_score[3].metrics)\n",
    "            print(sorted_by_score[4].metrics)\n",
    "            print(f\"{gen}::@@@@@@@@@@@@@@@@@@@@@\")\n",
    "            #-----------\n",
    "            indiv0 = np.array(sorted_by_score[0].bitstring).reshape(H,W)\n",
    "            indiv1 = np.array(sorted_by_score[1].bitstring).reshape(H,W)\n",
    "            indiv2 = np.array(sorted_by_score[2].bitstring).reshape(H,W)\n",
    "            indiv3 = np.array(sorted_by_score[3].bitstring).reshape(H,W)\n",
    "            indiv4 = np.array(sorted_by_score[4].bitstring).reshape(H,W)\n",
    "\n",
    "            for i, indiv in [('score0', indiv0), ('score1', indiv1), ('score2', indiv2), ('score3', indiv3), ('score4', indiv4)]:\n",
    "                img = np.zeros((H*4,W*4,3), np.uint8)\n",
    "                for x in range(indiv.shape[0]):\n",
    "                    for y in range(indiv.shape[1]):\n",
    "                        if indiv[x,y] == 1:\n",
    "                            img[x*4:x*4+4,y*4:y*4+4] = [255,0,0]\n",
    "                        else:\n",
    "                            img[x*4:x*4+4,y*4:y*4+4] = [0,255,0]\n",
    "\n",
    "                cv2.imwrite(f'./debug/visualize/this_{i}.jpg',img)\n",
    "            #-----------\n",
    "            \n",
    "            self.get_next_population()\n",
    "            # for p in self.population:\n",
    "            #     p.metrics = None\n",
    "            SAVE_STEP = 2\n",
    "            if gen % SAVE_STEP == 0:\n",
    "                ## SAVE THE PLOTS AFTER EVERY 2 iterations\n",
    "                ##------------------------\n",
    "                ## SAVE THE ACC DROP AND FILTERS DROPPED\n",
    "                fig, axs = plt.subplots(4, 1, figsize=(15,15), dpi=400)\n",
    "                axs[0].plot(np.arange(0,len(save_acc_drop)*SAVE_STEP,SAVE_STEP), save_acc_drop, linestyle='--', marker='o', color='b')\n",
    "                axs[0].set_xlabel('generations')\n",
    "                axs[0].set_ylabel('accuracy drop %')\n",
    "                \n",
    "                axs[1].plot(np.arange(0,len(save_fil_drop)*SAVE_STEP,SAVE_STEP), save_fil_drop, linestyle='--', marker='o', color='b')\n",
    "                axs[1].set_xlabel('generations')\n",
    "                axs[1].set_ylabel('filters dropped %')\n",
    "\n",
    "                save_acc_drop.append(sorted_by_score[0].metrics['accuracy_drop'])\n",
    "                save_fil_drop.append(sorted_by_score[0].metrics['per_filters_dropped'])\n",
    "\n",
    "                ##------------------------\n",
    "                ## SAVE THE FLOP DROP\n",
    "\n",
    "                pruned_model = make_model(sorted_by_score[0].bitstring, self.base_model)\n",
    "\n",
    "                random_input = self.dataset_test[np.random.randint(len(self.dataset_test))][0]\n",
    "                random_input = random_input.unsqueeze(0)\n",
    "                random_input = random_input.to('cuda')\n",
    "\n",
    "                base_flops_analysis = FlopCountAnalysis(self.base_model, random_input)\n",
    "                pruned_flops_analysis = FlopCountAnalysis(pruned_model, random_input)\n",
    "\n",
    "                base_flops = base_flops_analysis.total()\n",
    "                pruned_flops = pruned_flops_analysis.total()\n",
    "\n",
    "                # print('BASE   FLOPS:',base_flops)\n",
    "                # print('PRUNED FLOPS:',pruned_flops)\n",
    "\n",
    "                # print('FLOP DROP:', f'{(base_flops - pruned_flops)*100/base_flops}%')\n",
    "\n",
    "                save_flop_drop.append((base_flops - pruned_flops)*100/base_flops)\n",
    "\n",
    "                axs[2].plot(np.arange(0,len(save_flop_drop)*SAVE_STEP,SAVE_STEP), save_flop_drop, linestyle='--', marker='o', color='b')\n",
    "                axs[2].set_xlabel('generations')\n",
    "                axs[2].set_ylabel('FLOP dropped %')\n",
    "\n",
    "                ##------------------------\n",
    "                ## SAVE THE PARAM DROP\n",
    "\n",
    "                base_params_count = calculate_params(self.base_model)\n",
    "                pruned_params_count = calculate_params(pruned_model)\n",
    "                # print((base_params_count - pruned_params_count)*100 / base_params_count)\n",
    "                save_params_drop.append((base_params_count - pruned_params_count)*100 / base_params_count)\n",
    "\n",
    "                axs[3].plot(np.arange(0,len(save_params_drop)*SAVE_STEP,SAVE_STEP), save_params_drop, linestyle='--', marker='o', color='b')\n",
    "                axs[3].set_xlabel('generations')\n",
    "                axs[3].set_ylabel('Parameters dropped %')\n",
    "            \n",
    "                plt.savefig('./debug/save_svhn.png', bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "                pickle.dump(self.population, open('./checkpoints/pruned_population_ckpt/last_svhn_gen.p', 'wb'))\n",
    "                pickle.dump((save_acc_drop,save_fil_drop,save_flop_drop,save_params_drop), open('./checkpoints/pruned_plot_ckpt/last_svhn_plot.p', 'wb'))\n",
    "\n",
    "            gen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned_model, indiv = pickle.load(open('./models/pruned_models/pruned_vgg16_cifar10_model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      "==========\n",
      "gen:0, popsize:50\n",
      "==========\n",
      "{'score': -221.19247276422587, 'accuracy_drop': 2.2127578251907636, 'num_filters_dropped': 56, 'per_filters_dropped': 1.0174418604651163, 'accuracy': 96.38259824999659}\n",
      "{'score': 1001.0537790697674, 'accuracy_drop': 1.1534733882086385, 'num_filters_dropped': 58, 'per_filters_dropped': 1.0537790697674418, 'accuracy': 97.44188268697872}\n",
      "{'score': -403.0288963248117, 'accuracy_drop': 4.031014101041535, 'num_filters_dropped': 56, 'per_filters_dropped': 1.0174418604651163, 'accuracy': 94.56434197414582}\n",
      "{'score': -273.75489345364826, 'accuracy_drop': 2.738304871889369, 'num_filters_dropped': 53, 'per_filters_dropped': 0.9629360465116279, 'accuracy': 95.85705120329798}\n",
      "{'score': 1000.9265988372093, 'accuracy_drop': 1.579371254624121, 'num_filters_dropped': 51, 'per_filters_dropped': 0.9265988372093024, 'accuracy': 97.01598482056323}\n",
      "{'score': -629.7705367093159, 'accuracy_drop': 6.298374216798393, 'num_filters_dropped': 60, 'per_filters_dropped': 1.0901162790697674, 'accuracy': 92.29698185838896}\n",
      "{'score': 1000.890261627907, 'accuracy_drop': 1.5261340213221928, 'num_filters_dropped': 49, 'per_filters_dropped': 0.8902616279069767, 'accuracy': 97.06922205386516}\n",
      "{'score': 1000.7267441860465, 'accuracy_drop': 1.404643924812646, 'num_filters_dropped': 40, 'per_filters_dropped': 0.7267441860465116, 'accuracy': 97.19071215037471}\n",
      "{'score': -220.36891647986096, 'accuracy_drop': 2.2045674816058494, 'num_filters_dropped': 59, 'per_filters_dropped': 1.0719476744186047, 'accuracy': 96.3907885935815}\n",
      "{'score': -477.8327873400537, 'accuracy_drop': 4.7790654817969624, 'num_filters_dropped': 60, 'per_filters_dropped': 1.0901162790697674, 'accuracy': 93.81629059339039}\n",
      "{'score': -362.0843350728738, 'accuracy_drop': 3.621496921795867, 'num_filters_dropped': 49, 'per_filters_dropped': 0.8902616279069767, 'accuracy': 94.97385915339149}\n",
      "{'score': -217.38348197837507, 'accuracy_drop': 2.1745362217945114, 'num_filters_dropped': 47, 'per_filters_dropped': 0.8539244186046512, 'accuracy': 96.42081985339284}\n",
      "{'score': -640.0122070826076, 'accuracy_drop': 6.400753511609807, 'num_filters_dropped': 57, 'per_filters_dropped': 1.035610465116279, 'accuracy': 92.19460256357755}\n",
      "{'score': -412.4444162168945, 'accuracy_drop': 4.1252030522680485, 'num_filters_dropped': 59, 'per_filters_dropped': 1.0719476744186047, 'accuracy': 94.4701530229193}\n",
      "{'score': 1000.7994186046511, 'accuracy_drop': 1.8837790245300852, 'num_filters_dropped': 44, 'per_filters_dropped': 0.7994186046511628, 'accuracy': 96.71157705065727}\n",
      "{'score': -549.6522856270113, 'accuracy_drop': 5.497085602741038, 'num_filters_dropped': 48, 'per_filters_dropped': 0.872093023255814, 'accuracy': 93.09827047244632}\n",
      "{'score': -330.12266964975896, 'accuracy_drop': 3.3020735219842408, 'num_filters_dropped': 62, 'per_filters_dropped': 1.126453488372093, 'accuracy': 95.29328255320311}\n",
      "{'score': -206.04832871916133, 'accuracy_drop': 2.061236468869879, 'num_filters_dropped': 50, 'per_filters_dropped': 0.9084302325581395, 'accuracy': 96.53411960631747}\n",
      "{'score': -215.07022665276418, 'accuracy_drop': 2.151330248303921, 'num_filters_dropped': 42, 'per_filters_dropped': 0.7630813953488372, 'accuracy': 96.44402582688343}\n",
      "{'score': -226.92314109529576, 'accuracy_drop': 2.270090230285163, 'num_filters_dropped': 58, 'per_filters_dropped': 1.0537790697674418, 'accuracy': 96.32526584490219}\n",
      "{'score': -336.1347410907118, 'accuracy_drop': 3.362136041606945, 'num_filters_dropped': 58, 'per_filters_dropped': 1.0537790697674418, 'accuracy': 95.23322003358041}\n",
      "{'score': -225.957160938187, 'accuracy_drop': 2.2605348294360965, 'num_filters_dropped': 65, 'per_filters_dropped': 1.1809593023255813, 'accuracy': 96.33482124575126}\n",
      "{'score': -302.5550889116022, 'accuracy_drop': 3.0263319546254905, 'num_filters_dropped': 56, 'per_filters_dropped': 1.0174418604651163, 'accuracy': 95.56902412056186}\n",
      "{'score': 1000.9629360465116, 'accuracy_drop': 0.6538624295289139, 'num_filters_dropped': 53, 'per_filters_dropped': 0.9629360465116279, 'accuracy': 97.94149364565844}\n",
      "{'score': -263.90720732860916, 'accuracy_drop': 2.6400207488704126, 'num_filters_dropped': 66, 'per_filters_dropped': 1.1991279069767442, 'accuracy': 95.95533532631694}\n",
      "{'score': -333.54642832662364, 'accuracy_drop': 3.33619995358805, 'num_filters_dropped': 54, 'per_filters_dropped': 0.9811046511627907, 'accuracy': 95.2591561215993}\n",
      "{'score': 1001.1082848837209, 'accuracy_drop': 1.422389669246627, 'num_filters_dropped': 61, 'per_filters_dropped': 1.1082848837209303, 'accuracy': 97.17296640594073}\n",
      "{'score': -441.93876457731903, 'accuracy_drop': 4.420055421324932, 'num_filters_dropped': 53, 'per_filters_dropped': 0.9629360465116279, 'accuracy': 94.17530065386242}\n",
      "{'score': 1001.0356104651163, 'accuracy_drop': 1.4647064444353504, 'num_filters_dropped': 57, 'per_filters_dropped': 1.035610465116279, 'accuracy': 97.130649630752}\n",
      "{'score': -618.1592111915978, 'accuracy_drop': 6.182344349345456, 'num_filters_dropped': 67, 'per_filters_dropped': 1.217296511627907, 'accuracy': 92.4130117258419}\n",
      "{'score': -219.01419324396593, 'accuracy_drop': 2.1909169089643257, 'num_filters_dropped': 52, 'per_filters_dropped': 0.9447674418604651, 'accuracy': 96.40443916622303}\n",
      "{'score': -382.82236184518354, 'accuracy_drop': 3.8289856259470127, 'num_filters_dropped': 58, 'per_filters_dropped': 1.0537790697674418, 'accuracy': 94.76637044924034}\n",
      "{'score': 1000.9265988372093, 'accuracy_drop': 1.9643174031150608, 'num_filters_dropped': 51, 'per_filters_dropped': 0.9265988372093024, 'accuracy': 96.6310386720723}\n",
      "{'score': 1000.9629360465116, 'accuracy_drop': 1.6913059502846153, 'num_filters_dropped': 53, 'per_filters_dropped': 0.9629360465116279, 'accuracy': 96.90405012490274}\n",
      "{'score': 1000.7630813953489, 'accuracy_drop': 0.6852587466044184, 'num_filters_dropped': 42, 'per_filters_dropped': 0.7630813953488372, 'accuracy': 97.91009732858294}\n",
      "{'score': -218.33311237357196, 'accuracy_drop': 2.184091622643564, 'num_filters_dropped': 51, 'per_filters_dropped': 0.9265988372093024, 'accuracy': 96.41126445254379}\n",
      "{'score': 1000.8175872093024, 'accuracy_drop': 1.374612665001294, 'num_filters_dropped': 45, 'per_filters_dropped': 0.8175872093023255, 'accuracy': 97.22074341018606}\n",
      "{'score': 1000.9629360465116, 'accuracy_drop': 1.4728967880202646, 'num_filters_dropped': 53, 'per_filters_dropped': 0.9629360465116279, 'accuracy': 97.12245928716709}\n",
      "{'score': -332.9921978253607, 'accuracy_drop': 3.3307397245314547, 'num_filters_dropped': 60, 'per_filters_dropped': 1.0901162790697674, 'accuracy': 95.2646163506559}\n",
      "{'score': -407.0984567326319, 'accuracy_drop': 4.071965818966106, 'num_filters_dropped': 76, 'per_filters_dropped': 1.380813953488372, 'accuracy': 94.52339025622125}\n",
      "{'score': -232.25165841726096, 'accuracy_drop': 2.3233274635870913, 'num_filters_dropped': 55, 'per_filters_dropped': 0.9992732558139535, 'accuracy': 96.27202861160026}\n",
      "{'score': 1000.9811046511628, 'accuracy_drop': 1.6653698622657203, 'num_filters_dropped': 54, 'per_filters_dropped': 0.9811046511627907, 'accuracy': 96.92998621292163}\n",
      "{'score': 1000.7449127906976, 'accuracy_drop': 1.7254323818884245, 'num_filters_dropped': 41, 'per_filters_dropped': 0.7449127906976745, 'accuracy': 96.86992369329893}\n",
      "{'score': -290.80784891306797, 'accuracy_drop': 2.908937029908401, 'num_filters_dropped': 61, 'per_filters_dropped': 1.1082848837209303, 'accuracy': 95.68641904527895}\n",
      "{'score': -262.27909027660934, 'accuracy_drop': 2.623640061700584, 'num_filters_dropped': 59, 'per_filters_dropped': 1.0719476744186047, 'accuracy': 95.97171601348677}\n",
      "{'score': -223.79961880891022, 'accuracy_drop': 2.2386939132096586, 'num_filters_dropped': 47, 'per_filters_dropped': 0.8539244186046512, 'accuracy': 96.3566621619777}\n",
      "{'score': -394.96812800979114, 'accuracy_drop': 3.9504757224565594, 'num_filters_dropped': 61, 'per_filters_dropped': 1.1082848837209303, 'accuracy': 94.6448803527308}\n",
      "{'score': 1001.3263081395348, 'accuracy_drop': 1.5534351666052402, 'num_filters_dropped': 73, 'per_filters_dropped': 1.3263081395348837, 'accuracy': 97.04192090858211}\n",
      "{'score': -341.43439975826493, 'accuracy_drop': 3.4153732749088874, 'num_filters_dropped': 76, 'per_filters_dropped': 1.380813953488372, 'accuracy': 95.17998280027847}\n",
      "{'score': -210.41378190275753, 'accuracy_drop': 2.1049183013227406, 'num_filters_dropped': 52, 'per_filters_dropped': 0.9447674418604651, 'accuracy': 96.49043777386461}\n",
      "GEN:0::@@@@@@@@@@@@@@@@@@@@\n",
      "{'score': 1001.3263081395348, 'accuracy_drop': 1.5534351666052402, 'num_filters_dropped': 73, 'per_filters_dropped': 1.3263081395348837, 'accuracy': 97.04192090858211}\n",
      "{'score': 1001.1082848837209, 'accuracy_drop': 1.422389669246627, 'num_filters_dropped': 61, 'per_filters_dropped': 1.1082848837209303, 'accuracy': 97.17296640594073}\n",
      "{'score': 1001.0537790697674, 'accuracy_drop': 1.1534733882086385, 'num_filters_dropped': 58, 'per_filters_dropped': 1.0537790697674418, 'accuracy': 97.44188268697872}\n",
      "{'score': 1001.0356104651163, 'accuracy_drop': 1.4647064444353504, 'num_filters_dropped': 57, 'per_filters_dropped': 1.035610465116279, 'accuracy': 97.130649630752}\n",
      "{'score': 1000.9811046511628, 'accuracy_drop': 1.6653698622657203, 'num_filters_dropped': 54, 'per_filters_dropped': 0.9811046511627907, 'accuracy': 96.92998621292163}\n",
      "0::@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG(make_layers(\n",
    "    [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
    "          512, 512, 512, 512, 'M']\n",
    "))\n",
    "\n",
    "base_model.load_state_dict(torch.load('models/vgg19_svhn_base_model'))\n",
    "base_model = base_model.to('cuda')\n",
    "\n",
    "myGA = GeneticAlgo(\n",
    "    num_genes = 5504,\n",
    "    num_parents = 10,\n",
    "    base_model = base_model\n",
    ")\n",
    "\n",
    "# acc_drop, fil_drop, flop_drop, params_drop = pickle.load(open('./checkpoints/pruned_plot_ckpt/last_plot_bckp.p','rb'))\n",
    "# myGA.population = pickle.load(open('./checkpoints/pruned_population_ckpt/last_gen_bckp.p','rb'))\n",
    "# gen = int(open('./checkpoints/last_gen','r').read())\n",
    "# myGA.run(save_acc_drop=acc_drop, save_fil_drop=fil_drop, save_flop_drop=flop_drop, save_params_drop=params_drop, gen=gen)\n",
    "\n",
    "myGA.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTP_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
