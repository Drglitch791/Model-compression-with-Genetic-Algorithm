{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cifar10tools import test, make_model, VGG, make_layers, calculate_params\n",
    "from svhn_tools import test, make_model, VGG, make_layers, calculate_params\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     35\u001b[0m pruned_model \u001b[38;5;241m=\u001b[39m make_model(best_indiv\u001b[38;5;241m.\u001b[39mbitstring, base_model)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# dataset_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     transforms.ToTensor(),\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# ]))\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSVHN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.485\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.456\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.406\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.229\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.225\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     48\u001b[0m     dataset_test,\n\u001b[1;32m     49\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m pruned_acc, base_acc \u001b[38;5;241m=\u001b[39m test(dataloader, [pruned_model, base_model])\n",
      "File \u001b[0;32m~/miniconda3/envs/MTP3/lib/python3.10/site-packages/torchvision/datasets/svhn.py:76\u001b[0m, in \u001b[0;36mSVHN.__init__\u001b[0;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# import here rather than at top of file because this is\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# an optional dependency for torchvision\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msio\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# reading(loading) mat file as array\u001b[39;00m\n\u001b[1;32m     79\u001b[0m loaded_mat \u001b[38;5;241m=\u001b[39m sio\u001b[38;5;241m.\u001b[39mloadmat(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "class Individual():\n",
    "    def __init__(self, bitstring) -> None:\n",
    "        self.bitstring = bitstring\n",
    "        self.metrics = None\n",
    "    \n",
    "    def mutate(self):\n",
    "        mp_list = [0.0002]\n",
    "        mutation_prob = mp_list[np.random.randint(len(mp_list))]\n",
    "        self.metrics = None\n",
    "        self\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "# base_model = VGG(make_layers(\n",
    "#     cfg['D'] ## vgg1g\n",
    "# ))\n",
    "base_model = VGG(make_layers(\n",
    "    cfg['E'] ## vgg19\n",
    "))\n",
    "# base_model.load_state_dict(torch.load('models/vgg16_cifar10_base_model'))\n",
    "base_model.load_state_dict(torch.load('models/vgg19_svhn_base_model'))\n",
    "base_model = base_model.to('cuda')\n",
    "\n",
    "## Test the last saved model\n",
    "population = pickle.load(open('./checkpoints/pruned_population_ckpt/last_svhn_gen.p','rb'))\n",
    "# population = pickle.load(open('./checkpoints/pruned_population_ckpt/last_gen_best_vgg16.p','rb'))\n",
    "best_indiv = max(population, key=lambda x: x.metrics['score'])\n",
    "\n",
    "pruned_model = make_model(best_indiv.bitstring, base_model)\n",
    "\n",
    "# dataset_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ]))\n",
    "\n",
    "dataset_test = datasets.SVHN(root='./data', split='test', download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=2048, shuffle=True,\n",
    "    num_workers=4, pin_memory=True)\n",
    "pruned_acc, base_acc = test(dataloader, [pruned_model, base_model])\n",
    "print(pruned_acc, base_acc)\n",
    "print(f'ACC DROP:{(base_acc-pruned_acc)*100/base_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter drop of the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE PRAMS:20554826\n",
      "PRUNED PARAMS:19199962\n",
      "PARAMS DROP:6.591464213805556%\n"
     ]
    }
   ],
   "source": [
    "base_params_count = calculate_params(base_model)\n",
    "pruned_params_count = calculate_params(pruned_model)\n",
    "print(f'BASE PRAMS:{base_params_count}')\n",
    "print(f'PRUNED PARAMS:{pruned_params_count}')\n",
    "print(f'PARAMS DROP:{(base_params_count - pruned_params_count)*100 / base_params_count}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate FLOPS of pruned and base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::max_pool2d encountered 5 time(s)\n",
      "Unsupported operator aten::max_pool2d encountered 5 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE   FLOPS: 398660608\n",
      "PRUNED FLOPS: 379488084\n",
      "FLOP DROP: 4.809234625960336%\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "random_input = dataset_test[np.random.randint(len(dataset_test))][0]\n",
    "random_input = random_input.unsqueeze(0)\n",
    "random_input = random_input.to('cuda')\n",
    "\n",
    "base_flops_analysis = FlopCountAnalysis(base_model, random_input)\n",
    "pruned_flops_analysis = FlopCountAnalysis(pruned_model, random_input)\n",
    "\n",
    "base_flops = base_flops_analysis.total()\n",
    "pruned_flops = pruned_flops_analysis.total()\n",
    "\n",
    "print('BASE   FLOPS:',base_flops)\n",
    "print('PRUNED FLOPS:',pruned_flops)\n",
    "\n",
    "print('FLOP DROP:', f'{(base_flops - pruned_flops)*100/base_flops}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0705659729951549\n"
     ]
    }
   ],
   "source": [
    "## Compression ratio\n",
    "CR = (base_params_count/pruned_params_count)\n",
    "print(CR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTP_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
